# AI安全与社会计算

## 概述

工具的安全性是我们可以放心使用一种工具并大规模推广的前提，这种安全性的本质是确保被操作对象的行为与操作者的长期利益一致¹。然而，安全是相对的，对一个操作者安全的行为，有可能对另一个操作者的利益有冲突。一些典型的案例都发生在军事领域，例如，核武器的拥有和使用，对于拥有国来说，可能是维护其国家安全和地位的重要手段，但对于其他国家来说，可能是威胁和挑衅，引发核扩散和核恐怖主义的风险²；网络战争的发动，对于攻击方来说，可能是实现其战略目标和利益的有效方式，但对于被攻击方来说，可能是造成其国家安全和社会稳定的严重危机³；无人机的使用，对于部署方来说，可能是提高其军事效能和降低其伤亡的有效手段，但对于目标方来说，可能是侵犯其主权和人权的严重行为⁴。

这种由于操作者间利益冲突所导致的安全相对性，在过往的AI研究中并没有受到足够的重视。究其原因，是因为AI在过往的使用中往往被局限在一个相对较小的范围，单个“操作者-被操作对象”形成的小系统往往不会涉及与其它系统进行交互。例如，在已经存在的计算机视觉应用场景中，人脸识别、车牌识别、图像分割等技术，AI检测器的准确性、鲁棒性在一般情况下并不会因为使用者的不同而巨大的区分。

然而，这种系统的相对隔绝，随着AI的广泛使用，逐渐被打破。仍以计算机视觉应用为例，随着AI检测器的广泛使用，对于这些检测器的攻击，例如数据投毒、对抗样本等手段也开始不断发展，试图欺骗或者破坏AI检测器的功能⁵，此外，这些检测器训练所依赖的数据，导致的隐私侵犯、歧视性规则等问题也不断凸显，例如，人脸识别技术可能被用于非法监控、身份盗窃、人群分类等目的，造成个人隐私和社会公平的损害⁶，研究者们开始意识到，AI安全并不仅仅是一个单纯的技术问题，而有可能会与复杂的社会行为相关联。

这种AI安全的相对性和复杂性，随着ChatGPT等大语言模型的发展，开始被不断放大。例如，大语言模型可能会生成虚假或者有害的信息，如谣言、诽谤、色情、暴力等内容，影响社会舆论和道德秩序⁷；大语言模型可能会泄露训练数据中的敏感信息，如个人身份、隐私、商业机密等内容，威胁数据所有者的权益⁸；大语言模型可能会被恶意利用，如伪造新闻、制造网络水军、编写恶意代码等目的，危害社会安全和网络安全⁹。因此，对AI安全研究，有必要从系统和社会的角度出发，考虑AI技术、使用者、社会环境三者之间复杂的关联关系，从而给出更优的解决方案，实现更大的、或至少是帕累托改进的社会福利。

对社会系统，遵循对于系统的研究方法论，一般分为三个主要的方面：传感器、系统建模、控制器。传感器是指能够感知系统内外部状态和变化的设备或者机制，如温度计、摄像头、GPS等；系统建模是指能够描述系统的结构和行为的数学或者逻辑模型，如微分方程、有限状态机、神经网络等；控制器是指能够根据系统的目标和约束，对系统的输入或者输出进行调节的设备或者机制，如温控器、自动驾驶、反馈控制等。

基于同样的方法论，我们认为，对于AI安全的研究也可以划分为三个主要内容：

1. 社会信号传感器。为了实现更好的AI安全机制，应该如何激励、采集、处理社会信号？社会信号是指能够反映社会系统中各个主体的行为、偏好、信念、情绪等信息的数据或者信号，如语言、表情、姿态、声音、眼神等。社会信号传感器是指能够获取和分析社会信号的AI技术或者系统，如自然语言处理、情感分析、行为识别、社会网络分析等。社会信号传感器的作用是能够让AI系统更好地理解社会系统的状态和动态，从而提高其安全性和适应性。
2. 面向人机共生的建模方法。为了实现更好的AI安全机制，应该如何更好地建模、仿真存在大量Agent的未来社会运行机制？人机共生是指人类和AI系统之间形成一种互补、协作、共进的关系，共同构成一个更高层次的社会系统。人机共生的建模方法是指能够描述人机共生社会系统的结构和行为的数学或者逻辑模型，如博弈论、多智能体系统、复杂网络等。人机共生的建模方法的作用是能够让AI系统更好地预测和评估社会系统的未来状态和结果，从而提高其安全性和效率。
3. 面向AI的社会控制方法。为了实现更好的AI安全机制，应该如何采用去中心化、投票机制等新手段，实现有效的社会管控，以及如何利用联邦学习等技术，对AI技术本身的安全性进行改进？社会控制是指能够根据社会系统的目标和约束，对社会系统的输入或者输出进行调节的设备或者机制，如法律、道德、契约、惩罚等。面向AI的社会控制方法是指能够针对AI系统的特点和挑战，设计和实施更有效的社会控制手段的AI技术或者系统，如区块链、群智、信用系统等。面向AI的社会控制方法的作用是能够让AI系统更好地遵守社会系统的规则和价值，从而提高其安全性和可信度。